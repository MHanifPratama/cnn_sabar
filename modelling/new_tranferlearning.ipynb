{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "import random\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications.vgg16 import preprocess_input, VGG16\n",
    "from keras.models import Model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from keras_applications.imagenet_utils import _obtain_input_shape\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Activation, concatenate, Dropout\n",
    "from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D\n",
    "from tensorflow.python.keras.utils.data_utils import get_file\n",
    "\n",
    "from tensorflow.keras.utils import get_source_inputs\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "\n",
    "    def load_data(input_directory,\n",
    "                  train_directory,\n",
    "                  input_img_size: int = 64,\n",
    "                  mode_kelas=\"sparse\"):\n",
    "\n",
    "        CATEGORY = ['training', 'validation']\n",
    "        data_generator = ImageDataGenerator(rescale=1/255,\n",
    "                                  rotation_range=40,\n",
    "                                  width_shift_range=0.2,\n",
    "                                  height_shift_range=0.2,\n",
    "                                  shear_range=0.2,\n",
    "                                  zoom_range=0.2,\n",
    "                                  horizontal_flip=True,\n",
    "                                  fill_mode='nearest')\n",
    "        train_dataset = data_generator.flow_from_directory(train_directory, (input_img_size, input_img_size), class_mode=mode_kelas)\n",
    "\n",
    "        output = []\n",
    "\n",
    "        for category in CATEGORY:\n",
    "            path = os.path.join(input_directory, category)\n",
    "            print(path)\n",
    "            images = []\n",
    "            labels = []\n",
    "\n",
    "            for folder in os.listdir(path):\n",
    "                label = folder\n",
    "\n",
    "                for file in os.listdir(os.path.join(path, folder)):\n",
    "                    img_path = os.path.join(os.path.join(path, folder), file)\n",
    "                    image = cv2.imread(img_path)\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    image = cv2.resize(image, (input_img_size, input_img_size))\n",
    "                    images.append(image)\n",
    "                    labels.append(label)\n",
    "\n",
    "            images = np.array(images)\n",
    "            labels = np.array(labels)\n",
    "            output.append((images, labels))\n",
    "            return output\n",
    "\n",
    "    def datagen(input_directory, input_img_size: int, mode_kelas=\"sparse\"):\n",
    "        data_generator = ImageDataGenerator(rescale=1/255,\n",
    "                                  rotation_range=40,\n",
    "                                  width_shift_range=0.2,\n",
    "                                  height_shift_range=0.2,\n",
    "                                  shear_range=0.2,\n",
    "                                  zoom_range=0.2,\n",
    "                                  horizontal_flip=True,\n",
    "                                  fill_mode='nearest')\n",
    "        output = data_generator.flow_from_directory(input_directory, (input_img_size, input_img_size), class_mode=mode_kelas)\n",
    "        return output\n",
    "\n",
    "    def apply_preprocessing(dataset_folder,\n",
    "                            output_folder,\n",
    "                            val_split: float = 0.4,\n",
    "                            apply_aug: bool = True):\n",
    "\n",
    "        for folder in [output_folder, os.path.join(output_folder, 'training'), os.path.join(output_folder, 'validation')]:\n",
    "            if not os.path.exists(folder):\n",
    "                os.makedirs(folder)\n",
    "\n",
    "    # Parameter untuk ukuran dataset validation\n",
    "        validation_split = val_split\n",
    "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "        # apply_augmentation\n",
    "        def apply_augmentation(image):\n",
    "\n",
    "            if random.random() < 0.5:\n",
    "                # Flip gambar secara horizontal\n",
    "                image = cv2.flip(image, 1)\n",
    "\n",
    "            if random.random() < 0.5:\n",
    "                # Menaikkan dan menurunkan kecerahan\n",
    "                alpha = random.uniform(0.5, 1.5)\n",
    "                image = cv2.multiply(image, np.array([alpha]))\n",
    "\n",
    "            if random.random() < 0.5:\n",
    "                # Menaikkan dan menurunkan kontras\n",
    "                alpha = random.uniform(0.5, 1.5)\n",
    "                grey = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                grey = cv2.cvtColor(grey, cv2.COLOR_GRAY2BGR)\n",
    "                image = cv2.addWeighted(image, alpha, grey, 1 - alpha, 0)\n",
    "\n",
    "            # if random.random() < 0.5:\n",
    "            #     # Rotasi 90 derajat\n",
    "            #     image = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "            return image\n",
    "\n",
    "        for class_name in os.listdir(dataset_folder):\n",
    "            class_folder = os.path.join(dataset_folder, class_name)\n",
    "\n",
    "            training_class_folder = os.path.join(output_folder, 'training', class_name)\n",
    "            validation_class_folder = os.path.join(output_folder, 'validation', class_name)\n",
    "            for folder in [training_class_folder, validation_class_folder]:\n",
    "                if not os.path.exists(folder):\n",
    "                    os.makedirs(folder)\n",
    "\n",
    "            image_files = os.listdir(class_folder)\n",
    "            random.shuffle(image_files)\n",
    "            num_validation_images = int(len(image_files) * validation_split)\n",
    "\n",
    "            for i, image_name in enumerate(image_files) :\n",
    "                image_path = os.path.join(class_folder, image_name)\n",
    "\n",
    "                if i < num_validation_images:\n",
    "                    output_image_path = os.path.join(validation_class_folder, image_name)\n",
    "                else:\n",
    "                    output_image_path = os.path.join(training_class_folder, image_name)\n",
    "\n",
    "                image = cv2.imread(image_path)\n",
    "                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=16, minSize=(64, 64))\n",
    "\n",
    "                if len(faces)>0:\n",
    "                    for (x, y, w, h) in faces:\n",
    "                        cv2.rectangle(image, (x, y), (x+w, y+h), (0, 0, 0), 0)\n",
    "                        cropped = image[y:y + h, x:x + w]\n",
    "                try:\n",
    "                    cv2.imwrite(output_image_path, cropped)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            if apply_aug == True:\n",
    "                for i, image_name in enumerate(image_files) :\n",
    "                    image_path = os.path.join(class_folder, image_name)\n",
    "                    output_image_path = os.path.join(training_class_folder, image_name)\n",
    "                    image = cv2.imread(image_path)\n",
    "                    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=16, minSize=(64, 64))\n",
    "                    if len(faces)>0:\n",
    "                        for (x, y, w, h) in faces:\n",
    "                            cv2.rectangle(image, (x, y), (x+w, y+h), (0, 0, 0), 0)\n",
    "                            cropped = image[y:y + h, x:x + w]\n",
    "                    try:\n",
    "                        augmented = apply_augmentation(cropped)\n",
    "                        cv2.imwrite(output_image_path, augmented)\n",
    "                    except:\n",
    "                        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_resnet_v2 import preprocess_input, InceptionResNetV2\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(\"D:\\CNN-Tim\\dataset\")\n",
    "train_path = os.path.join(data_path, 'training')\n",
    "validation_path = os.path.join(data_path, 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(preprocess_input)\n",
    "validate_gen = ImageDataGenerator(preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 246 images belonging to 11 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 76 images belonging to 11 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = train_gen.flow_from_directory(train_path, \n",
    "                                      target_size = (224,224), \n",
    "                                      batch_size = 5)\n",
    "validate_gen = validate_gen.flow_from_directory(validation_path, \n",
    "                                            target_size = (224,224), \n",
    "                                            batch_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model1 = tf.keras.models.Sequential([\n",
    "    # 1st conv\n",
    "    tf.keras.layers.Conv2D(32, (5, 5), strides=(5, 5), activation='relu', input_shape=(224, 224, 3)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D(2, strides=(2, 2)),  # Adjust strides here\n",
    "    \n",
    "    # 2nd conv\n",
    "    tf.keras.layers.Conv2D(64, (11, 11), strides=(1, 1), activation='relu', padding=\"same\"),  # Change strides to (1, 1)\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    \n",
    "    # 3rd conv\n",
    "    tf.keras.layers.Conv2D(128, (11, 11), strides=(1, 1), activation='relu', padding=\"same\"),  # Change strides to (1, 1)\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    \n",
    "    # 4th conv\n",
    "    tf.keras.layers.Conv2D(64, (11, 11), strides=(1, 1), activation='relu', padding=\"same\"),  # Change strides to (1, 1)\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    \n",
    "    # 5th Conv\n",
    "    tf.keras.layers.Conv2D(32, (11, 11), strides=(1, 1), activation='relu', padding=\"same\"),  # Change strides to (1, 1)\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    \n",
    "    tf.keras.layers.MaxPooling2D(2, strides=(2, 2)),  # Adjust strides here\n",
    "\n",
    "    # To Flatten layer\n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    # To FC layer 1\n",
    "    tf.keras.layers.Dense(1024, activation='relu'),\n",
    "    \n",
    "    # Add dropout\n",
    "    tf.keras.layers.Dropout(0.8),\n",
    "    \n",
    "    # To FC layer 2\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " sequential_5 (Sequential)   (None, 3)                 6451107   \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 3)                 0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 11)                44        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,451,151\n",
      "Trainable params: 6,450,511\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Create the MobileNetV2 base model with its default input shape (224, 224, 3)\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False)\n",
    "\n",
    "# Set every layer in the base_model as non-trainable\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a new input layer with the desired input shape\n",
    "new_input_shape = (224, 224, 3)\n",
    "input_layer = tf.keras.layers.Input(shape=new_input_shape)\n",
    "\n",
    "# Create your custom model (model1) and pass the input through it\n",
    "\n",
    "model1 = tf.keras.models.Sequential([\n",
    "    # 1st conv\n",
    "    tf.keras.layers.Conv2D(32, (5, 5), strides=(5, 5), activation='relu', input_shape=(224, 224, 3)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D(2, strides=(2, 2)),  # Adjust strides here\n",
    "    \n",
    "    # 2nd conv\n",
    "    tf.keras.layers.Conv2D(64, (11, 11), strides=(1, 1), activation='relu', padding=\"same\"),  # Change strides to (1, 1)\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    \n",
    "    # 3rd conv\n",
    "    tf.keras.layers.Conv2D(128, (11, 11), strides=(1, 1), activation='relu', padding=\"same\"),  # Change strides to (1, 1)\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    \n",
    "    # 4th conv\n",
    "    tf.keras.layers.Conv2D(64, (11, 11), strides=(1, 1), activation='relu', padding=\"same\"),  # Change strides to (1, 1)\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    \n",
    "    # 5th Conv\n",
    "    tf.keras.layers.Conv2D(32, (11, 11), strides=(1, 1), activation='relu', padding=\"same\"),  # Change strides to (1, 1)\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    \n",
    "    tf.keras.layers.MaxPooling2D(2, strides=(2, 2)),  # Adjust strides here\n",
    "\n",
    "    # To Flatten layer\n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    # To FC layer 1\n",
    "    tf.keras.layers.Dense(1024, activation='relu'),\n",
    "    \n",
    "    # Add dropout\n",
    "    tf.keras.layers.Dropout(0.8),\n",
    "    \n",
    "    # To FC layer 2\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "# Pass the input through model1\n",
    "x = model1(input_layer)\n",
    "\n",
    "\n",
    "# Add any additional custom layers as needed\n",
    "x = Flatten()(x)\n",
    "x = Dense(train_gen.num_classes, activation='softmax')(x)\n",
    "\n",
    "# Create the final combined model\n",
    "combined_model = Model(inputs=input_layer, outputs=x)\n",
    "combined_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The last dimension of the inputs to a Dense layer should be defined. Found None. Full input shape received: (None, None)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\CNN-Tim\\cnn_sabar\\modelling\\new_tranferlearning.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CNN-Tim/cnn_sabar/modelling/new_tranferlearning.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m base_out \u001b[39m=\u001b[39m base_model\u001b[39m.\u001b[39moutput\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CNN-Tim/cnn_sabar/modelling/new_tranferlearning.ipynb#X14sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m base_out \u001b[39m=\u001b[39m Flatten()(base_out)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/CNN-Tim/cnn_sabar/modelling/new_tranferlearning.ipynb#X14sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m base_out \u001b[39m=\u001b[39m Dense(train_gen\u001b[39m.\u001b[39;49mnum_classes, activation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msoftmax\u001b[39;49m\u001b[39m'\u001b[39;49m)(base_out)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CNN-Tim/cnn_sabar/modelling/new_tranferlearning.ipynb#X14sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m model1 \u001b[39m=\u001b[39m Model(inputs\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39minput, outputs\u001b[39m=\u001b[39mbase_out)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CNN-Tim/cnn_sabar/modelling/new_tranferlearning.ipynb#X14sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m model\u001b[39m.\u001b[39msummary()\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\layers\\core\\dense.py:148\u001b[0m, in \u001b[0;36mDense.build\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    146\u001b[0m last_dim \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mdimension_value(input_shape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m    147\u001b[0m \u001b[39mif\u001b[39;00m last_dim \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 148\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    149\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe last dimension of the inputs to a Dense layer \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    150\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mshould be defined. Found None. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    151\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFull input shape received: \u001b[39m\u001b[39m{\u001b[39;00minput_shape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    152\u001b[0m     )\n\u001b[0;32m    153\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_spec \u001b[39m=\u001b[39m InputSpec(min_ndim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, axes\u001b[39m=\u001b[39m{\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m: last_dim})\n\u001b[0;32m    154\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_weight(\n\u001b[0;32m    155\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mkernel\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    156\u001b[0m     shape\u001b[39m=\u001b[39m[last_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munits],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    161\u001b[0m     trainable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    162\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: The last dimension of the inputs to a Dense layer should be defined. Found None. Full input shape received: (None, None)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Create the MobileNetV2 base model with its default input shape (224, 224, 3)\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False)\n",
    "\n",
    "# Set every layer in the base_model as non-trainable\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "\n",
    "# Combine your models\n",
    "\n",
    "  # Set x to the input layer\n",
    "model = Model(inputs=base_model.input, outputs=base_model.layers[-4].output)\n",
    "base_out = base_model.output\n",
    "base_out = Flatten()(base_out)\n",
    "base_out = Dense(train_gen.num_classes, activation='softmax')(base_out)\n",
    "model1 = Model(inputs=model.input, outputs=base_out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(128,128,3))\n",
    "# x = base_model.output\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "# x = Dense(train_gen.num_classes, activation='softmax')(x) \n",
    "\n",
    "# model = Model(inputs=base_model.input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
